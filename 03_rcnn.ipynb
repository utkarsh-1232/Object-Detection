{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20e8d54-e8e9-433c-a8d7-6706e626e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import *\n",
    "from src.rois import *\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torchvision.ops import nms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastai.vision.all import DataLoaders, OptimWrapper, Learner\n",
    "from selectivesearch import selective_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9715c089-c7b5-4bb9-bb49-a3f02376dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader('data')\n",
    "df, id2label = loader.load_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0d8b36-55a6-48e9-9023-712061c0fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rois_ss(img):\n",
    "    img_area = img.shape[1]*img.shape[2]\n",
    "    _, regions = selective_search(img.permute((1,2,0)), scale=200, min_size=100)\n",
    "    rois = torch.tensor([r['rect'] for r in regions])\n",
    "    sizes = torch.tensor([r['size'] for r in regions])\n",
    "    mask = (sizes>0.05*img_area) & (sizes<img_area)\n",
    "    return ROIs(rois[mask, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41984337-6a8b-413e-9909-0695bfc0ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = train_test_split(df.loc[:1000], test_size=0.2)\n",
    "\n",
    "train_ds = ObjectDataset(df=train_df, extract_rois=extract_rois_ss)\n",
    "eval_ds = ObjectDataset(df=eval_df, extract_rois=extract_rois_ss)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=train_ds.collate_fn, pin_memory=True)\n",
    "eval_dl = DataLoader(eval_ds, batch_size=4, shuffle=False, collate_fn=eval_ds.collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87378df9-2a7f-40ee-af7b-880a4702f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, eval_dl)\n",
    "dls.n_inp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a47829-f439-4e36-a6ac-9fe8e3a96d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25088"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "vgg16.classifier[0].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecdf6af8-683a-411a-af91-1524667f51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.img_encoder = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        for param in self.img_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.img_encoder.eval()\n",
    "        encode_dim = self.img_encoder.classifier[0].in_features\n",
    "        self.img_encoder.classifier = nn.Sequential()\n",
    "\n",
    "        self.cls_head = nn.Linear(encode_dim, n_classes)\n",
    "        self.cls_loss = nn.CrossEntropyLoss()\n",
    "        self.reg_head = nn.Sequential(\n",
    "             nn.Linear(encode_dim, 512), nn.ReLU(),\n",
    "             nn.Linear(512, 4), nn.Sigmoid(),\n",
    "        )\n",
    "        self.reg_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, crops):\n",
    "        features = self.img_encoder(crops)\n",
    "        probs = self.cls_head(features)\n",
    "        bbox = self.reg_head(features)\n",
    "        return probs, bbox\n",
    "\n",
    "    def calc_loss(self, preds, ids, offsets, beta=0.1):\n",
    "        probs, bbox = preds\n",
    "        cls_loss = self.cls_loss(probs, ids)\n",
    "        mask = ids!=0\n",
    "        bbox, offsets = bbox[mask], offsets[mask]\n",
    "        reg_loss = self.reg_loss(bbox, offsets) if len(mask)>0 else torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "        return beta*cls_loss + (1-beta)*reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c6d909d-499b-4c6c-9ace-e280b42ba6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCNN(len(id2label))\n",
    "opt_func = partial(OptimWrapper, opt=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b1d8b1d-898f-4828-91fa-5e67f86430cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=model.calc_loss, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0c318-f4fa-410a-82a1-9299360d7e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/3 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='13' class='' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      22.41% [13/58 03:41&lt;12:46 nan]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awesomeville/miniforge3/lib/python3.12/site-packages/skimage/feature/texture.py:385: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(n_epoch=3, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f484e0-917d-41e8-aa19-017fd92fc6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
