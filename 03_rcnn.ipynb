{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20e8d54-e8e9-433c-a8d7-6706e626e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core import *\n",
    "from src.rois import *\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms.functional import resized_crop\n",
    "\n",
    "from fastai.vision.all import DataLoaders, OptimWrapper, Learner, Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9715c089-c7b5-4bb9-bb49-a3f02376dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader('data')\n",
    "df, id2label, id2img = loader.load_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd57e5e-63d5-4760-8335-c879fef5eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "sample_df = df.sample(n)\n",
    "train_df, eval_df = sample_df.iloc[:int(0.8*n)], sample_df.iloc[int(0.8*n):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d5a7519-b265-4cad-930d-721b3e062aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_data = [\n",
    "    {'id':i,'image_id':row['image_id'],'bbox':bbox,'category_id':cat_id,'iscrowd':0,'area':0}\n",
    "    for i, row in eval_df.iterrows()\n",
    "    for bbox, cat_id in zip(row['bbox'], row['category_id'])\n",
    "]\n",
    "images_data = [\n",
    "    {'id':row['image_id'], 'file_name':f'{str(row['image_id']).zfill(12)}.jpg'}\n",
    "    for _, row in eval_df.iterrows()\n",
    "]\n",
    "cat_data = [{'id':k, 'name':v} for k,v in id2label.items() if k!=0]\n",
    "with open('tmp/eval_gt.json', 'w') as f:\n",
    "    json.dump({'categories':cat_data,'annotations':ann_data,'images':images_data}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449c2785-94c9-46f1-8e5f-81ca9b249d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awesomeville/miniforge3/lib/python3.12/site-packages/skimage/feature/texture.py:385: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [01:10<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "train_res = train_df.progress_apply(get_annotated_rois, axis=1, id2img=id2img)\n",
    "\n",
    "train_img_ids = torch.cat([row[0] for row in train_res])\n",
    "train_rois = torch.cat([row[1] for row in train_res])\n",
    "train_roi_ids = torch.cat([row[2] for row in train_res])\n",
    "train_offsets = torch.cat([row[3] for row in train_res])\n",
    "train_img_dims = torch.cat([row[4] for row in train_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16827778-0927-44b7-8809-d6c59267c9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:18<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_res = eval_df.progress_apply(get_annotated_rois, axis=1, id2img=id2img)\n",
    "\n",
    "eval_img_ids = torch.cat([row[0] for row in eval_res])\n",
    "eval_rois = torch.cat([row[1] for row in eval_res])\n",
    "eval_roi_ids = torch.cat([row[2] for row in eval_res])\n",
    "eval_offsets = torch.cat([row[3] for row in eval_res])\n",
    "eval_img_dims = torch.cat([row[4] for row in eval_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1452a2cb-c708-4ee6-86d2-22ec4e904ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNNDataset(Dataset):\n",
    "    def __init__(self, img_ids, rois, roi_ids, offsets, img_dims, id2img, crop_size=(224,224)):\n",
    "        self.img_ids, self.rois, self.roi_ids = img_ids, rois, roi_ids\n",
    "        self.offsets, self.img_dims = offsets, img_dims\n",
    "        self.id2img = id2img\n",
    "        self.crop_size = crop_size\n",
    "        self.img_tfms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img = Image.open(self.id2img[img_id.item()]).convert('RGB')\n",
    "        img = self.img_tfms(img)\n",
    "        offset = self.offsets[idx]/self.img_dims[idx]\n",
    "        x_min, y_min, w, h = self.rois[idx].int().tolist()\n",
    "        crop = resized_crop(img, top=y_min, left=x_min, height=h, width=w, size=self.crop_size)\n",
    "        \n",
    "        return crop, img_id, self.rois[idx], self.roi_ids[idx], offset, self.img_dims[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce7029b-0e51-40dc-9bed-d1d832bcd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RCNNDataset(train_img_ids, train_rois, train_roi_ids, train_offsets, train_img_dims, id2img)\n",
    "eval_ds = RCNNDataset(eval_img_ids, eval_rois, eval_roi_ids, eval_offsets, eval_img_dims, id2img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba335c6b-b0c3-4c01-81e5-ed64f6649f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, pin_memory=True)\n",
    "eval_dl = DataLoader(eval_ds, batch_size=32, shuffle=False, pin_memory=True)\n",
    "\n",
    "dls = DataLoaders(train_dl, eval_dl)\n",
    "dls.n_inp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a47829-f439-4e36-a6ac-9fe8e3a96d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25088"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "vgg16.classifier[0].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a43e164-424a-4b6e-a647-d9b281d5c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.img_encoder = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        for param in self.img_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.img_encoder.eval()\n",
    "        encode_dim = self.img_encoder.classifier[0].in_features\n",
    "        self.img_encoder.classifier = nn.Sequential()\n",
    "\n",
    "        self.cls_head = nn.Linear(encode_dim, n_classes)\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(encode_dim, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 4), nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, crops):\n",
    "        features = self.img_encoder(crops)\n",
    "        probs = self.cls_head(features)\n",
    "        bbox = self.reg_head(features)\n",
    "        return probs, bbox\n",
    "\n",
    "    def calc_loss(self, preds, *targs, beta=0.2):\n",
    "        probs, pred_offsets = preds\n",
    "        _, _, ids, offsets, _ = targs\n",
    "        cls_loss = nn.CrossEntropyLoss()(probs, ids)\n",
    "        reg_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        mask = ids!=0\n",
    "        if torch.sum(mask)>0:\n",
    "            reg_loss = nn.MSELoss()(pred_offsets[mask], offsets[mask])\n",
    "            \n",
    "        return beta*cls_loss + (1-beta)*reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08bd4880-3b3b-4338-b772-d29b957f06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "class mAP(Metric):\n",
    "    def __init__(self, gt_path, pred_path):\n",
    "        self.gt_path, self.pred_path = gt_path, pred_path\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        with open(self.pred_path, \"w\") as f:\n",
    "            json.dump([], f, indent=4)\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        probs, pred_offsets = learn.pred\n",
    "        scores, pred_ids = probs.max(dim=1)\n",
    "        img_ids, rois, ids, _, img_dims  = learn.y\n",
    "        mask = ids!=0\n",
    "        pred_offsets = pred_offsets*img_dims\n",
    "        pred_bbs = rois+pred_offsets\n",
    "        \n",
    "        self.write_to_file(img_ids[mask], pred_ids[mask], pred_bbs[mask], scores[mask])\n",
    "\n",
    "    def write_to_file(self, img_ids, ids, pred_bbs, scores):\n",
    "        iterables = [img_ids.tolist(), ids.tolist(), pred_bbs.tolist(), scores.tolist()]\n",
    "        new_anns = [\n",
    "            {'image_id':img_id, 'category_id':id, 'bbox':bbox, 'score':score}\n",
    "            for img_id, id, bbox, score in zip(*iterables)\n",
    "        ]\n",
    "        with open(self.pred_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        data += new_anns\n",
    "        with open(self.pred_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "            \n",
    "    @property\n",
    "    def value(self):\n",
    "        coco_gt = COCO(self.gt_path)\n",
    "        coco_pred = coco_gt.loadRes(self.pred_path)\n",
    "        cocoEval = COCOeval(coco_gt, coco_pred, 'bbox')\n",
    "        cocoEval.evaluate()\n",
    "        cocoEval.accumulate()\n",
    "        cocoEval.summarize()\n",
    "        return cocoEval.stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb4f5f95-04e2-4a21-9850-5171ef01f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCNN(len(id2label))\n",
    "opt_func = partial(OptimWrapper, opt=torch.optim.Adam)\n",
    "mAP_metric = mAP('tmp/eval_gt.json', 'tmp/eval_preds.json')\n",
    "\n",
    "learn = Learner(dls, model, loss_func=model.calc_loss, opt_func=opt_func, metrics=mAP_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86b0c318-f4fa-410a-82a1-9299360d7e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>m_ap</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.136659</td>\n",
       "      <td>0.160650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>08:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(n_epoch=1, lr_max=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494c68a-8400-4a52-a5d6-ae8af85e02ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
